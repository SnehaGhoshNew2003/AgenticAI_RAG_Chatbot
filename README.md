# AgenticAI_RAG_Chatbot

A **Retrieval-Augmented Generation (RAG)** based chatbot that answers user queries from PDF documents using **LangGraph workflows**, **Google Gemini**, and **semantic search**.

---

##  Key Features

- **PDF Ingestion** â€“ Extracts and indexes text from PDF documents  
- **Semantic Search** â€“ Retrieves relevant content using vector similarity  
- **Conversational Q&A** â€“ Natural language question answering  
- **Intent Classification** â€“ Routes queries (greeting, summary, question)  
- **Document Summarization** â€“ Generates concise overviews  
- **FastAPI Backend** â€“ High-performance REST API  
- **Streamlit Frontend** â€“ Clean and interactive user interface  

---
## âš™ï¸ Environment Setup

Create a `.env` file in the project root directory:

```env
GOOGLE_API_KEY=your_google_api_key
CHROMA_DIR=./chroma_db
LLM_MODEL=gemini-2.5-flash
HF_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```
---

##  System Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streamlit   â”‚ Frontend UI
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI     â”‚ Backend API
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ LangGraph   â”‚ Workflow Engine
â”‚ (Classify â†’ â”‚
â”‚ Retrieve â†’  â”‚
â”‚ Generate)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           â”‚               â”‚
â–¼           â–¼               â–¼
ChromaDB   Gemini LLM      HF Embeddings
(Vector)   (LLM)           (Semantic)


## ğŸ“¦ Install Dependencies

Install all required Python packages using:

```bash
pip install -r requirements.txt
```

## ğŸ“„ PDF Ingestion

Place your ebook PDF in the project root directory and run:

```bash
python -m app.ingest
```

### This step performs the following:

- Loads the PDF document  
- Splits the content into semantic text chunks  
- Generates embeddings using HuggingFace models  
- Stores vector embeddings in ChromaDB for efficient retrieval  

---

## ğŸš€ Backend (FastAPI)

Start the FastAPI backend server using:

```bash
uvicorn backend.main:app --reload
```

### The backend is responsible for:

- Handling user queries  
- Managing LangGraph agent state  
- Performing document retrieval  
- Generating grounded responses using retrieved context  

---

## ğŸ¨ Frontend (Streamlit)

Launch the Streamlit frontend interface with:

```bash
streamlit run frontend/front.py
```
### ğŸ¨ Frontend Capabilities

The frontend allows users to:

- Ask questions related to the ebook  
- Receive context-aware, grounded answers  
- Ensure responses are generated strictly from retrieved document content  

---

## ğŸ§  Tech Stack

- **LangChain** â€“ LLM orchestration  
- **LangGraph** â€“ Agentic workflow and state management  
- **ChromaDB** â€“ Vector database for similarity search  
- **FastAPI** â€“ Backend API  
- **Streamlit** â€“ Interactive frontend UI  
- **HuggingFace Embeddings** â€“ Text vectorization  
- **Google Gemini LLM** â€“ Response generation  

---
## ğŸ“¦ Cloning the Project

To get started with this project, you need to clone the repository to your local machine. Make sure you have **Git** installed.

```bash
# Clone the repository
git clone https://github.com/yourusername/rag-ai-chatbot.git

# Navigate into the project directory
cd rag-ai-chatbot
```
After cloning, you can proceed to set up your environment, install dependencies, and configure API keys as described in the setup instructions.

